{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Exercises\n",
    "\n",
    "* We'll use scikit-learn and tensorflow to predict Ad Click-Through.\n",
    "* The goal is to try different models that give us the highest AUC testing.\n",
    "* Also, it's important to see if we can train the model with 10 million\n",
    "samples or if it's too much for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1:\n",
    "In the logistic regression-based click-through prediction project, can you also tweak hyperparameters such as penalty, eta0, and alpha in the SGDClassifier model?\n",
    "\n",
    "What is the highest testing AUC you are able to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Read the first 100,000 rows of the dataset\n",
    "n_rows = 100_000\n",
    "df = pd.read_csv(\"./dataset/train.csv\", nrows=n_rows)\n",
    "\n",
    "# Drop unnecessary columns and prepare X and Y\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "Y = df['click'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transforming them to One-Hot Encoded data\n",
    "* We will only train the model using 270,000 samples, 30,000 will be for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (90% - 10%)\n",
    "n_train = int(n_rows * 0.9)\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train].astype('float16')\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:].astype('float16')\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_enc = enc.fit_transform(X_train).toarray().astype('float16')\n",
    "X_test_enc = enc.transform(X_test).toarray().astype('float16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preparing the model \n",
    "* We'll tweak with penalty, eta0 and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Preparing the parameters to use with GridSearch\n",
    "parameters = {'penalty': ['l1', 'l2'],\n",
    "             'eta0': [1e-03, 1e-02, 1e-01],\n",
    "             'alpha': [1e-03, 1e-02,]}\n",
    "\n",
    "model = SGDClassifier(loss = \"log_loss\", fit_intercept=True, learning_rate='constant', verbose=1)\n",
    "\n",
    "model_grid = GridSearchCV(model, parameters, n_jobs=-1, cv=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.38, NNZs: 5566, Bias: -0.213428, T: 90000, Avg. loss: 0.429156\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.71, NNZs: 5566, Bias: -0.241317, T: 180000, Avg. loss: 0.420588\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.94, NNZs: 5566, Bias: -0.256211, T: 270000, Avg. loss: 0.418425\n",
      "Total training time: 3.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.12, NNZs: 5566, Bias: -0.282163, T: 360000, Avg. loss: 0.416963\n",
      "Total training time: 4.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.27, NNZs: 5566, Bias: -0.306288, T: 450000, Avg. loss: 0.416112\n",
      "Total training time: 5.52 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.38, NNZs: 5566, Bias: -0.327163, T: 540000, Avg. loss: 0.415356\n",
      "Total training time: 6.60 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.49, NNZs: 5566, Bias: -0.361351, T: 630000, Avg. loss: 0.414835\n",
      "Total training time: 7.67 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.58, NNZs: 5566, Bias: -0.379046, T: 720000, Avg. loss: 0.414428\n",
      "Total training time: 8.74 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.66, NNZs: 5566, Bias: -0.386419, T: 810000, Avg. loss: 0.414200\n",
      "Total training time: 9.81 seconds.\n",
      "Convergence after 9 epochs took 9.81 seconds\n",
      "{'alpha': 0.001, 'eta0': 0.001, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Training and getting the best parameters\n",
    "model_grid.fit(X_train_enc, Y_train)\n",
    "\n",
    "print(model_grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model: {'alpha': 0.001, 'eta0': 0.001, 'penalty': 'l2'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and evaluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC_AUC after training with 100,000 samples is: 0.724661623809706\n"
     ]
    }
   ],
   "source": [
    "## Predicting and evaluating\n",
    "from sklearn.metrics import roc_auc_score\n",
    "logistic_best = model_grid.best_estimator_\n",
    "\n",
    "pred = logistic_best.predict_proba(X_test_enc)\n",
    "print(f\"The ROC_AUC after training with 100,000 samples is: {roc_auc_score(Y_test, pred[:,1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "Can you try to use more training samples, for instance, 10 million samples, in the online learning solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining our variables\n",
    "n_rows = 10_000 * 1100\n",
    "df = pd.read_csv(\"./dataset/train.csv\", nrows = n_rows)\n",
    "# Splitting the features from the target\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "y = df['click'].values\n",
    "\n",
    "# Splitting in training and testing\n",
    "Y = df['click'].values\n",
    "n_train = 100000 * 100\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc.fit(X_train.toarray().)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the SGD model. max_iter is set to 1 for online learning\n",
    "sgd_lr_online = SGDClassifier(loss = 'log_loss', penalty = 'l2',\n",
    "                              fit_intercept = True, max_iter = 1,\n",
    "                              learning_rate = 'constant', eta0 = 0.001, alpha = 0.001, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.43, NNZs: 5725, Bias: -0.211223, T: 100000, Avg. loss: 0.429137\n",
      "Total training time: 5.57 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1.74, NNZs: 7439, Bias: -0.251675, T: 100000, Avg. loss: 0.419017\n",
      "Total training time: 5.57 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1.97, NNZs: 8559, Bias: -0.306295, T: 100000, Avg. loss: 0.398179\n",
      "Total training time: 5.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.15, NNZs: 9332, Bias: -0.342524, T: 100000, Avg. loss: 0.377457\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.30, NNZs: 10025, Bias: -0.375888, T: 100000, Avg. loss: 0.387828\n",
      "Total training time: 4.85 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.44, NNZs: 10530, Bias: -0.406709, T: 100000, Avg. loss: 0.410690\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.57, NNZs: 11125, Bias: -0.441446, T: 100000, Avg. loss: 0.397483\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.73, NNZs: 11572, Bias: -0.477114, T: 100000, Avg. loss: 0.379210\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.84, NNZs: 11880, Bias: -0.500645, T: 100000, Avg. loss: 0.379272\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.94, NNZs: 12243, Bias: -0.547446, T: 100000, Avg. loss: 0.369954\n",
      "Total training time: 4.84 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.06, NNZs: 12524, Bias: -0.583684, T: 100000, Avg. loss: 0.372853\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.13, NNZs: 12742, Bias: -0.621432, T: 100000, Avg. loss: 0.370768\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.17, NNZs: 13069, Bias: -0.650787, T: 100000, Avg. loss: 0.388311\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.21, NNZs: 13288, Bias: -0.686637, T: 100000, Avg. loss: 0.390599\n",
      "Total training time: 4.85 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.19, NNZs: 13619, Bias: -0.714715, T: 100000, Avg. loss: 0.400341\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.21, NNZs: 13857, Bias: -0.719348, T: 100000, Avg. loss: 0.406761\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.20, NNZs: 14150, Bias: -0.753373, T: 100000, Avg. loss: 0.405336\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.23, NNZs: 14364, Bias: -0.780977, T: 100000, Avg. loss: 0.404227\n",
      "Total training time: 4.95 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.25, NNZs: 14658, Bias: -0.830186, T: 100000, Avg. loss: 0.403720\n",
      "Total training time: 5.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.29, NNZs: 14896, Bias: -0.873017, T: 100000, Avg. loss: 0.395616\n",
      "Total training time: 5.63 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.30, NNZs: 15098, Bias: -0.880514, T: 100000, Avg. loss: 0.398758\n",
      "Total training time: 5.53 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.29, NNZs: 15349, Bias: -0.896838, T: 100000, Avg. loss: 0.419446\n",
      "Total training time: 5.30 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.28, NNZs: 15531, Bias: -0.926680, T: 100000, Avg. loss: 0.425520\n",
      "Total training time: 5.09 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 15729, Bias: -0.942988, T: 100000, Avg. loss: 0.453718\n",
      "Total training time: 5.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 15896, Bias: -0.966357, T: 100000, Avg. loss: 0.453236\n",
      "Total training time: 5.12 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.30, NNZs: 16070, Bias: -0.974185, T: 100000, Avg. loss: 0.450928\n",
      "Total training time: 5.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.30, NNZs: 16272, Bias: -0.998832, T: 100000, Avg. loss: 0.423608\n",
      "Total training time: 5.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 16426, Bias: -1.025989, T: 100000, Avg. loss: 0.431897\n",
      "Total training time: 5.20 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.33, NNZs: 16593, Bias: -1.019746, T: 100000, Avg. loss: 0.454940\n",
      "Total training time: 4.99 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.33, NNZs: 16741, Bias: -1.035616, T: 100000, Avg. loss: 0.451196\n",
      "Total training time: 5.20 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.34, NNZs: 16907, Bias: -1.065535, T: 100000, Avg. loss: 0.445509\n",
      "Total training time: 5.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.36, NNZs: 17056, Bias: -1.072057, T: 100000, Avg. loss: 0.445108\n",
      "Total training time: 4.98 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.36, NNZs: 17185, Bias: -1.073803, T: 100000, Avg. loss: 0.448613\n",
      "Total training time: 4.90 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 17301, Bias: -1.100433, T: 100000, Avg. loss: 0.446041\n",
      "Total training time: 4.82 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.33, NNZs: 17426, Bias: -1.112809, T: 100000, Avg. loss: 0.436980\n",
      "Total training time: 5.34 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.32, NNZs: 17530, Bias: -1.119946, T: 100000, Avg. loss: 0.426810\n",
      "Total training time: 5.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 17646, Bias: -1.143347, T: 100000, Avg. loss: 0.414779\n",
      "Total training time: 5.26 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 17755, Bias: -1.148729, T: 100000, Avg. loss: 0.398604\n",
      "Total training time: 5.04 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.26, NNZs: 17872, Bias: -1.174913, T: 100000, Avg. loss: 0.395770\n",
      "Total training time: 5.61 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.19, NNZs: 17985, Bias: -1.181882, T: 100000, Avg. loss: 0.415072\n",
      "Total training time: 5.48 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.12, NNZs: 18107, Bias: -1.189722, T: 100000, Avg. loss: 0.439084\n",
      "Total training time: 5.24 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.10, NNZs: 18218, Bias: -1.192319, T: 100000, Avg. loss: 0.437524\n",
      "Total training time: 5.48 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.10, NNZs: 18325, Bias: -1.175786, T: 100000, Avg. loss: 0.441785\n",
      "Total training time: 5.53 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.18, NNZs: 18450, Bias: -1.163580, T: 100000, Avg. loss: 0.441076\n",
      "Total training time: 5.43 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.20, NNZs: 18565, Bias: -1.170314, T: 100000, Avg. loss: 0.430942\n",
      "Total training time: 5.24 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.24, NNZs: 18691, Bias: -1.193273, T: 100000, Avg. loss: 0.418887\n",
      "Total training time: 5.24 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.27, NNZs: 18809, Bias: -1.215380, T: 100000, Avg. loss: 0.407878\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 18918, Bias: -1.228088, T: 100000, Avg. loss: 0.384115\n",
      "Total training time: 5.22 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.40, NNZs: 19041, Bias: -1.235302, T: 100000, Avg. loss: 0.359819\n",
      "Total training time: 5.25 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.41, NNZs: 19116, Bias: -1.247612, T: 100000, Avg. loss: 0.362130\n",
      "Total training time: 5.21 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.38, NNZs: 19214, Bias: -1.256040, T: 100000, Avg. loss: 0.388191\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.35, NNZs: 19300, Bias: -1.263567, T: 100000, Avg. loss: 0.393131\n",
      "Total training time: 5.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.35, NNZs: 19387, Bias: -1.267419, T: 100000, Avg. loss: 0.395477\n",
      "Total training time: 5.25 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.37, NNZs: 19508, Bias: -1.297786, T: 100000, Avg. loss: 0.430372\n",
      "Total training time: 5.08 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.35, NNZs: 19597, Bias: -1.289430, T: 100000, Avg. loss: 0.441711\n",
      "Total training time: 4.99 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.39, NNZs: 19717, Bias: -1.350461, T: 100000, Avg. loss: 0.389022\n",
      "Total training time: 5.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.49, NNZs: 19786, Bias: -1.350698, T: 100000, Avg. loss: 0.360995\n",
      "Total training time: 4.87 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.62, NNZs: 19857, Bias: -1.361954, T: 100000, Avg. loss: 0.359270\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.72, NNZs: 19978, Bias: -1.395301, T: 100000, Avg. loss: 0.354095\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.76, NNZs: 20066, Bias: -1.398391, T: 100000, Avg. loss: 0.351367\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.78, NNZs: 20131, Bias: -1.416083, T: 100000, Avg. loss: 0.351447\n",
      "Total training time: 4.87 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.80, NNZs: 20182, Bias: -1.425449, T: 100000, Avg. loss: 0.351996\n",
      "Total training time: 5.53 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.81, NNZs: 20227, Bias: -1.417743, T: 100000, Avg. loss: 0.352149\n",
      "Total training time: 5.50 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.76, NNZs: 20307, Bias: -1.432943, T: 100000, Avg. loss: 0.353366\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.72, NNZs: 20366, Bias: -1.451083, T: 100000, Avg. loss: 0.360708\n",
      "Total training time: 5.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.70, NNZs: 20409, Bias: -1.449903, T: 100000, Avg. loss: 0.353678\n",
      "Total training time: 5.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.68, NNZs: 20431, Bias: -1.463927, T: 100000, Avg. loss: 0.353436\n",
      "Total training time: 4.82 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.62, NNZs: 20483, Bias: -1.469144, T: 100000, Avg. loss: 0.352961\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.61, NNZs: 20546, Bias: -1.481724, T: 100000, Avg. loss: 0.348694\n",
      "Total training time: 4.82 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3.59, NNZs: 20589, Bias: -1.491700, T: 100000, Avg. loss: 0.348972\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.57, NNZs: 20617, Bias: -1.515423, T: 100000, Avg. loss: 0.348083\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.56, NNZs: 20687, Bias: -1.514056, T: 100000, Avg. loss: 0.351274\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.55, NNZs: 20743, Bias: -1.531812, T: 100000, Avg. loss: 0.350550\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.53, NNZs: 20778, Bias: -1.534392, T: 100000, Avg. loss: 0.353044\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.50, NNZs: 20805, Bias: -1.523599, T: 100000, Avg. loss: 0.355674\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.51, NNZs: 20864, Bias: -1.546064, T: 100000, Avg. loss: 0.350048\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.53, NNZs: 20914, Bias: -1.550158, T: 100000, Avg. loss: 0.341283\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.55, NNZs: 20946, Bias: -1.551214, T: 100000, Avg. loss: 0.342887\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.54, NNZs: 21009, Bias: -1.573574, T: 100000, Avg. loss: 0.380655\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.52, NNZs: 21076, Bias: -1.591370, T: 100000, Avg. loss: 0.439311\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.48, NNZs: 21160, Bias: -1.587424, T: 100000, Avg. loss: 0.436541\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.46, NNZs: 21218, Bias: -1.611800, T: 100000, Avg. loss: 0.437501\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.41, NNZs: 21278, Bias: -1.627216, T: 100000, Avg. loss: 0.424777\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.38, NNZs: 21317, Bias: -1.656594, T: 100000, Avg. loss: 0.421237\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.37, NNZs: 21399, Bias: -1.645915, T: 100000, Avg. loss: 0.428680\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.37, NNZs: 21452, Bias: -1.667134, T: 100000, Avg. loss: 0.423323\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.34, NNZs: 21525, Bias: -1.671199, T: 100000, Avg. loss: 0.414237\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.30, NNZs: 21586, Bias: -1.680440, T: 100000, Avg. loss: 0.415466\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.29, NNZs: 21681, Bias: -1.700798, T: 100000, Avg. loss: 0.400747\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.25, NNZs: 21737, Bias: -1.695197, T: 100000, Avg. loss: 0.399254\n",
      "Total training time: 4.76 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.22, NNZs: 21810, Bias: -1.718171, T: 100000, Avg. loss: 0.393132\n",
      "Total training time: 4.85 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.20, NNZs: 21862, Bias: -1.728708, T: 100000, Avg. loss: 0.397019\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.15, NNZs: 21926, Bias: -1.722883, T: 100000, Avg. loss: 0.408234\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.15, NNZs: 21999, Bias: -1.737208, T: 100000, Avg. loss: 0.410262\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.17, NNZs: 22047, Bias: -1.731359, T: 100000, Avg. loss: 0.427437\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.19, NNZs: 22109, Bias: -1.720805, T: 100000, Avg. loss: 0.447211\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.21, NNZs: 22187, Bias: -1.728446, T: 100000, Avg. loss: 0.458520\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.26, NNZs: 22252, Bias: -1.734864, T: 100000, Avg. loss: 0.469529\n",
      "Total training time: 5.60 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 22303, Bias: -1.732029, T: 100000, Avg. loss: 0.442787\n",
      "Total training time: 5.49 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.32, NNZs: 22415, Bias: -1.746682, T: 100000, Avg. loss: 0.438175\n",
      "Total training time: 5.26 seconds.\n",
      "--- 1530.0780300000001.3fs seconds ---\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "# Building a loop (0100 times). We need to specify the classes in online learning\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(1000):\n",
    "    x_train = X_train[i * 100_00: (i+1) * 10_000]\n",
    "    y_train = Y_train[i * 100_00: (i+1) * 10_000]\n",
    "    x_train_enc = enc.transform(x_train)\n",
    "    sgd_lr_online.partial_fit(x_train_enc.toarray(), y_train, classes = [0, 1])\n",
    "\n",
    "print(f\"--- {(timeit.default_timer() - start_time)}.3fs seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 100000000, AUC on testing set: 0.687\n"
     ]
    }
   ],
   "source": [
    "# Applying the trained model on the testing set, the final 100_000 samples\n",
    "x_test_enc = enc.transform(X_test)\n",
    "\n",
    "pred = sgd_lr_online.predict_proba(x_test_enc)[:, 1]\n",
    "print(f'Training samples: {n_train * 10}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: \n",
    "In the TensorFlow-based solution, can you tweak the learning rate, the number of training steps, and other hyperparameters to obtain a better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Read the first 300,000 rows of the dataset\n",
    "n_rows = 3_000_000\n",
    "df = pd.read_csv(\"./dataset/train.csv\", nrows=n_rows)\n",
    "\n",
    "# Drop unnecessary columns and prepare X and Y\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "Y = df['click'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transforming the data into One-Hot Encoded items: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type 'float8' not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m n_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_rows \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X[:n_train]\n\u001b[1;32m----> 4\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X[n_train:]\n\u001b[0;32m      6\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m Y[n_train:]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: data type 'float8' not understood"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (90% - 10%)\n",
    "n_train = int(n_rows * 0.9)\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train].astype('float8')\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:].astype('float8')\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_enc = enc.fit_transform(X_train).toarray().astype('single')\n",
    "X_test_enc = enc.transform(X_test).toarray().astype('single')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Using the Sequential model from Keras\n",
    "\n",
    "* While Keras is mainly used for building neural networks, it can also be used to create a logistic regression model.\n",
    "* In this case, the logistic regression model can be seen as a simple one-layer neural network with a sigmoid activation function.\n",
    "* When we compile and train this model, it essentially learns the weights and bias of a logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model using Keras\n",
    "# The Sequential model is a linear stack of layers in Keras, which is a popular deep learning library in Python.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(X_train_enc.shape[1],))\n",
    "])\n",
    "\n",
    "# Set up the learning rate and optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, )\n",
    "\n",
    "# Compile the model with binary cross entropy loss since it's a binary classification problem\n",
    "# Set the metric as the ROC_AUC\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with 1000 sample-batches\n",
    "batch_size = 1000\n",
    "epochs = 18\n",
    "model.fit(X_train_enc, Y_train, batch_size = batch_size, epochs = epochs, verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step: Making predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, auc = model.evaluate(X_test_enc, Y_test, verbose = 0)\n",
    "print(f'AUC with 2,700,000 training samples oon testing set: {auc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
