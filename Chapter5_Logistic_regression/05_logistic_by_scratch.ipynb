{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Logistic Regression algorithm\n",
    "* We'll implement the logistic regression algorithm using functions.\n",
    "* We will only use 10_000 samples from the Ad Click-Through dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Function that computes the prediction $\\hat{y}(x)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Computing the initial prediction of y_hat.\n",
    "\n",
    "import numpy as np\n",
    "def sigmoid(input):\n",
    "    return 1.0/(1 + np.exp(-input))\n",
    "\n",
    "def compute_prediction(X, weights):\n",
    "    \"\"\"\n",
    "    Computes the prediction y_hat based on the current weights\n",
    "    @param X: A numpy array of features.\n",
    "    @param weights: The computed weights for the Logistic Regression algorithm\n",
    "    @return: y_hat, a prediction (values that range between 0 and 1)\n",
    "    \"\"\"\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = sigmoid(z)\n",
    "    return predictions\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Function to update the weights:\n",
    "$w: = w + n\\frac{1}{m} \\sum^{m}_{i=1}{(y^i-\\hat{y}(z^i))x^i}$\n",
    "\n",
    "This function takes as input the training data (X_train and y_train), the current weights of the model, and the learning rate. It computes the predictions using the current weights, calculates the gradient, and updates the weights according to the gradient descent update rule. The updated weights are then returned.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def update_weights_gd(X_train, y_train, weights, learning_rate):\n",
    "    \"\"\"\n",
    "\n",
    "    @param X_train: (numpy array): The feature matrix of shape (m, n), where m is the number of samples and n is the number of features.\n",
    "    @param y_train: (numpy array): The target vector of shape (m,).\n",
    "    @param weights:(numpy array): The weight vector of the linear model of shape (n,).\n",
    "    @param learning_rate: The learning rate for gradient descent.\n",
    "    @return: numpy array: The updated weight vector of shape (n,).\n",
    "    \"\"\"\n",
    "    # Compute the predictions using the current weights\n",
    "    predictions = compute_prediction(X_train, weights)\n",
    "\n",
    "    # Calculate the gradient by computing the dot product of the transpose of the feature matrix and the error (y_train - predictions)\n",
    "    weights_delta = np.dot(X_train.T, y_train - predictions)\n",
    "\n",
    "    # Get the number of samples in the training data\n",
    "    m = y_train.shape[0]\n",
    "    # Update the weights by adding the scaled gradient, where the scaling factor is the learning rate divided by the number of samples (m)\n",
    "    weights += learning_rate / float(m) * weights_delta\n",
    "\n",
    "    # Return the updated weights\n",
    "    return weights\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: The function calculating the cost J(w)\n",
    "$J(w) = \\frac{1}{m}\\sum^{m}_{i = 1}{-[y^i \\log{\\hat{y}(x^i)} + (1 - y^i)\n",
    "\\log{(1-\\hat{y}(x^i))}]}$\n",
    "\n",
    "This function takes as input the feature matrix X, the target vector y, and the weight vector of the logistic regression model. It computes the predictions using the current weights and then calculates the cross-entropy cost (also known as the loss) based on the given formula."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def compute_cost(X, y, weights):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy cost (loss) for a logistic regression model.\n",
    "\n",
    "    @param X: numpy array): The feature matrix of shape (m, n), where m is\n",
    "    the number of samples and n is the number of features.\n",
    "    @param y:(numpy array): The target vector of shape (m,).\n",
    "    @param weights:( vnumpy array): The weight vector of the logistic\n",
    "    regression model of shape (n,).\n",
    "\n",
    "    @return float: The cross-entropy cost.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the predictions using the current weights\n",
    "    predictions = compute_prediction(X, weights)\n",
    "\n",
    "    # Calculate the cross-entropy cost using the given formula\n",
    "    cost = np.mean(-y * np.log(predictions) - (1-y) * np.log(1 - predictions))\n",
    "\n",
    "    # Return the computed cost\n",
    "    return cost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Connecting the functions\n",
    "* Updating the weights vector in each iteration\n",
    "* Printing out the current cost for every 100 (this can be another value)\n",
    "iterations to ensure cost is decreasing and that things are on the right track.\n",
    "\n",
    "This function trains a logistic regression model using gradient descent. It takes as input the feature matrix X_train, the target vector y_train, the maximum number of iterations for gradient descent, the learning rate, and an optional parameter fit_intercept that indicates whether to fit the intercept term. If fit_intercept is True, an intercept term is added to the feature matrix. The weights are initialized to zeros, and gradient descent is performed for the specified number of iterations. The cost is checked and printed for every 100 iterations. The trained weight vector is returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, max_iter, learning_rate,\n",
    "                              fit_intercept = False):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using gradient descent\n",
    "\n",
    "    @param X_train: (numpy array): The feature matrix of shape (m, n), where m is the number of samples and n is the number of features.\n",
    "    @param y_train: (numpy array): The target vector of shape (m,).\n",
    "    @param max_iter: int: The maximum number of iterations for gradient descent.\n",
    "    @param learning_rate: (float): The learning rate for gradient descent.\n",
    "    @param fit_intercept:(bool, optional): Whether to fit the intercept term. Defaults to False.\n",
    "    @return:     numpy array: The trained weight vector of the logistic regression model of shape (n,).\n",
    "    \"\"\"\n",
    "\n",
    "    # Add an intercept term to the feature matrix if fit_intercept is True\n",
    "    if fit_intercept:\n",
    "        intercept = np.ones((X_train.shape[0], 1))\n",
    "        X_train = np.hstack((intercept, X_train))\n",
    "\n",
    "    # Initialize the weights to zeros\n",
    "    weights = np.zeros(X_train.shape[1])\n",
    "\n",
    "    # Perform gradient descent for the specified number of iterations\n",
    "    for iteration in range(max_iter):\n",
    "        weights = update_weights_gd(X_train, y_train, weights, learning_rate)\n",
    "        # Check the cost for every 100 iterations.\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Cost: {compute_cost(X_train, y_train, weights)}\")\n",
    "\n",
    "    # Return the trained weights\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final step: Predicting new inputs.\n",
    "The predict function is used to predict class probabilities using a logistic regression model. The input is a feature matrix X of shape (n_samples, n_features) and a weight vector weights of shape (n_features + 1,). The function first checks if the input matrix X has one less column than the length of the weight vector, which means it's missing the intercept term. If this is the case, the function adds an intercept column of ones to the input matrix X. Finally, the function calls the compute_prediction function with the updated X and weights to calculate the predicted class probabilities and returns the result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def predict(X, weights):\n",
    "    \"\"\"\n",
    "    Predict the class probabilities using the logistic regression model.\n",
    "\n",
    "    @param X: numpy array of shape (n_samples, n_features), input features.\n",
    "    @param weights: numpy array of shape (n_features + 1,), model weights (including the intercept term).\n",
    "    @return: numpy array of shape (n_samples,), predicted class probabilities.\n",
    "    \"\"\"\n",
    "    # Check if the number of columns in X is one less than the length of weights.\n",
    "    # If true, it means the intercept term is not included in X.\n",
    "    if X.shape[1] == weights.shape[0] - 1:\n",
    "        # Add an intercept column with all ones to X.\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((intercept, X))\n",
    "\n",
    "    # Compute the predicted class probabilities using the input features and the model weights.\n",
    "    return compute_prediction(X, weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting click-through"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "n_rows = 300_000\n",
    "df = pd.read_csv(\"./dataset/train.csv\", nrows = n_rows)\n",
    "\n",
    "# Splitting the column features from the target values\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "y = df['click'].values\n",
    "\n",
    "# We will only train the model using 10,000 samples\n",
    "n_train = 10000\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Performing one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "X_train_enc = enc.fit_transform(X_train)\n",
    "X_test_enc = enc.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.6820019456743648\n",
      "Cost: 0.4608619713011896\n",
      "Cost: 0.4503715555130051\n",
      "Cost: 0.4455503890097847\n",
      "Cost: 0.4420611414384596\n",
      "Cost: 0.4393702812833892\n",
      "Cost: 0.437228041454526\n",
      "Cost: 0.4354781787758496\n",
      "Cost: 0.43401801289720104\n",
      "Cost: 0.4327779028622343\n",
      "Cost: 0.4317091585700226\n",
      "Cost: 0.43077673019057455\n",
      "Cost: 0.42995469288423555\n",
      "Cost: 0.42922339559221634\n",
      "Cost: 0.4285676184571522\n",
      "Cost: 0.42797535312823465\n",
      "Cost: 0.4274369752561037\n",
      "Cost: 0.42694466897530536\n",
      "Cost: 0.42649201676958726\n",
      "Cost: 0.42607370031421204\n",
      "Cost: 0.42568527750493995\n",
      "Cost: 0.42532301300292674\n",
      "Cost: 0.4249837472238756\n",
      "Cost: 0.42466479353954484\n",
      "Cost: 0.4243638565943513\n",
      "Cost: 0.4240789667070855\n",
      "Cost: 0.42380842671759145\n",
      "Cost: 0.42355076859163654\n",
      "Cost: 0.42330471776471257\n",
      "Cost: 0.42306916368249065\n",
      "Cost: 0.4228431353432208\n",
      "Cost: 0.42262578090532044\n",
      "Cost: 0.4224163506180466\n",
      "Cost: 0.42221418248223747\n",
      "Cost: 0.4220186901637542\n",
      "Cost: 0.42182935277298567\n",
      "Cost: 0.42164570619560027\n",
      "Cost: 0.42146733571705797\n",
      "Cost: 0.4212938697294381\n",
      "Cost: 0.421124974346345\n",
      "Cost: 0.4209603487818472\n",
      "Cost: 0.42079972137401617\n",
      "Cost: 0.42064284615376507\n",
      "Cost: 0.42048949987620765\n",
      "Cost: 0.4203394794453518\n",
      "Cost: 0.4201925996741633\n",
      "Cost: 0.4200486913313148\n",
      "Cost: 0.4199075994336317\n",
      "Cost: 0.419769181749643\n",
      "Cost: 0.4196333074849702\n",
      "Cost: 0.419499856124741\n",
      "Cost: 0.41936871641193046\n",
      "Cost: 0.41923978544365365\n",
      "Cost: 0.4191129678700612\n",
      "Cost: 0.41898817518269\n",
      "Cost: 0.41886532508098934\n",
      "Cost: 0.41874434090732193\n",
      "Cost: 0.41862515114206933\n",
      "Cost: 0.418507688951615\n",
      "Cost: 0.4183918917829391\n",
      "Cost: 0.41827770099938916\n",
      "Cost: 0.41816506155289535\n",
      "Cost: 0.41805392168850386\n",
      "Cost: 0.41794423267762576\n",
      "Cost: 0.4178359485768415\n",
      "Cost: 0.417729026009493\n",
      "Cost: 0.41762342396762814\n",
      "Cost: 0.41751910363215045\n",
      "Cost: 0.41741602820928153\n",
      "Cost: 0.41731416278166233\n",
      "Cost: 0.4172134741726095\n",
      "Cost: 0.41711393082221093\n",
      "Cost: 0.41701550267409204\n",
      "Cost: 0.41691816107180946\n",
      "Cost: 0.41682187866394727\n",
      "Cost: 0.4167266293170827\n",
      "Cost: 0.41663238803588404\n",
      "Cost: 0.41653913088967376\n",
      "Cost: 0.41644683494486445\n",
      "Cost: 0.41635547820272995\n",
      "Cost: 0.4162650395420338\n",
      "Cost: 0.4161754986660808\n",
      "Cost: 0.41608683605380203\n",
      "Cost: 0.4159990329145223\n",
      "Cost: 0.4159120711460908\n",
      "Cost: 0.4158259332960885\n",
      "Cost: 0.4157406025258515\n",
      "Cost: 0.41565606257707494\n",
      "Cost: 0.41557229774078286\n",
      "Cost: 0.4154892928284704\n",
      "Cost: 0.4154070331452411\n",
      "Cost: 0.4153255044647785\n",
      "Cost: 0.415244693006007\n",
      "Cost: 0.41516458541130685\n",
      "Cost: 0.4150851687261618\n",
      "Cost: 0.4150064303801305\n",
      "Cost: 0.41492835816903717\n",
      "Cost: 0.41485094023829017\n",
      "Cost: 0.41477416506724385\n",
      "Cost: 0.41469802145452467\n",
      "--- 194.5640698000061.3fs seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Training the model over 10_000 iterations, learning rate of 0.01 and with bias\n",
    "# Calculating the time it takes to train the model\n",
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "weights = train_logistic_regression(X_train_enc.toarray(), y_train, max_iter\n",
    "= 10_000, learning_rate = 0.01, fit_intercept = True)\n",
    "print(f\"--- {(timeit.default_timer() - start_time)}.3fs seconds ---\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 10000, AUC on testing set: 0.703\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pred = predict(X_test_enc.toarray(), weights)\n",
    "print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(y_test, pred):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing Stochastic Gradient Descent\n",
    "* For each weight update, only one training sample is consumed, instead of\n",
    "the complete training set.\n",
    "* We just need to update the update_weights() and the\n",
    "train_logistic_regression() functions\n",
    "\n",
    "The function iterates through each data point in the training set, computes the prediction for the current data point using the current model weights, and calculates the gradient (weights_delta) for the current data point. It then updates the model weights using the learning rate and the calculated gradient.\n",
    "\n",
    "The updated model weights are returned at the end of the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def update_weights_sgd(X_train, y_train, weights, learning_rate):\n",
    "    \"\"\"\n",
    "\n",
    "    @param X_train: numpy array of shape (n_samples, n_features), input features.\n",
    "    @param y_train: numpy array of shape (n_samples,), target class labels.\n",
    "    @param weights: numpy array of shape (n_features + 1,), model weights (including the intercept term).\n",
    "    @param learning_rate: float, the learning rate for gradient descent.\n",
    "    @return: numpy array of shape (n_features + 1,), updated model weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through each data point in the training set (X_train, y_train)\n",
    "    for X_each, y_each in zip(X_train, y_train):\n",
    "        # Compute the prediction for the current data point using the current model weights\n",
    "        prediction = compute_prediction(X_each, weights)\n",
    "\n",
    "        # Calculate the gradient (weights_delta) for the current data point\n",
    "        weights_delta = X_each.T * (y_each - prediction)\n",
    "        # Update the model weights using the learning rate and the calculated gradient\n",
    "        weights += learning_rate* weights_delta\n",
    "\n",
    "    # Return the updated model weights\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_logistic_regression_sgd(X_train, y_train, max_iter, learning_rate,\n",
    "                                  fit_intercept = False):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "     @param X_train: numpy array of shape (n_samples, n_features), input features.\n",
    "    @param y_train: numpy array of shape (n_samples,), target class labels.\n",
    "    @param max_iter: int, the number of iterations for weight updates.\n",
    "    @param learning_rate: float, the learning rate for gradient descent.\n",
    "    @param fit_intercept: bool, whether to add an intercept term to the model.\n",
    "    @return: numpy array of shape (n_features + 1,), trained model weights (including the intercept term).\n",
    "\n",
    "    \"\"\"\n",
    "    # If fit_intercept is True, add an intercept term to the input features\n",
    "    if fit_intercept:\n",
    "        intercept = np.ones((X_train.shape[0], 1))\n",
    "        X_train = np.hstack((intercept, X_train))\n",
    "\n",
    "    # Initialize the weights with zeros\n",
    "    weights = np.zeros(X_train.shape[1])\n",
    "    for iteration in range(max_iter):\n",
    "        weights = update_weights_sgd(X_train, y_train, weights, learning_rate)\n",
    "        # Check the cost for every 2 iterations.\n",
    "        if iteration % 2 == 0:\n",
    "            print(f\"Cost: {compute_cost(X_train, y_train, weights)}\")\n",
    "\n",
    "    # Return the trained model weights\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using SGD and scaling the model\n",
    "* We will increase the amount of samples we'll train the model with since SGD\n",
    " can take it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "n_rows = 300_000\n",
    "df = pd.read_csv(\"./dataset/train.csv\", nrows = n_rows)\n",
    "\n",
    "# Splitting the column features from the target values\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "y = df['click'].values\n",
    "\n",
    "# We will only train the model using 100,000 samples\n",
    "n_train = 100_000\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "# Performing one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "X_train_enc = enc.fit_transform(X_train)\n",
    "X_test_enc = enc.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.4127864859625796\n",
      "Cost: 0.4078504597223988\n",
      "Cost: 0.40545733114863264\n",
      "Cost: 0.403811787845451\n",
      "Cost: 0.4025431351250833\n",
      "Cost: 0.4015053950669261\n",
      "Cost: 0.40062464023567285\n",
      "Cost: 0.39985799447134973\n",
      "Cost: 0.3991783043895136\n",
      "Cost: 0.398567258491007\n",
      "Cost: 0.39801190940990816\n",
      "Cost: 0.3975027566890244\n",
      "Cost: 0.39703261643081\n",
      "Cost: 0.3965959186801836\n",
      "Cost: 0.39618825187922974\n",
      "Cost: 0.39580605722362766\n",
      "Cost: 0.39544641765210864\n",
      "Cost: 0.3951069085504533\n",
      "Cost: 0.3947854898106667\n",
      "Cost: 0.39448042625798385\n",
      "Cost: 0.3941902279442516\n",
      "Cost: 0.39391360461804337\n",
      "Cost: 0.39364943048841494\n",
      "Cost: 0.39339671658454667\n",
      "Cost: 0.3931545888057168\n",
      "--- 103.14262400000007.3fs seconds ---\n",
      "Training samples: 100000, AUC on testing set: 0.730\n"
     ]
    }
   ],
   "source": [
    "# Training the model over 100_000 iterations, learning rate of 0.01 and with\n",
    "# bias\n",
    "# Calculating the time it takes to train the model\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "weights = train_logistic_regression_sgd(X_train_enc.toarray(), y_train,\n",
    "                                        max_iter = 50, learning_rate = 0.01,\n",
    "                                        fit_intercept = True)\n",
    "print(f\"--- {(timeit.default_timer() - start_time)}.3fs seconds ---\")\n",
    "\n",
    "# Predicting values and getting ROC AUC Score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pred = predict(X_test_enc.toarray(), weights)\n",
    "print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(y_test, pred):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
