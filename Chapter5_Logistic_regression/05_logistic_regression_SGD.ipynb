{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using SGDClassifier\n",
    "* We'll implement Logistic Regression using the SGDClassifier module of\n",
    "scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "* We'll set up 'log' as the loss parameter which indicates that the cost\n",
    "function is log loss.\n",
    "* Penalty is the regularization term to reduce overfitting.\n",
    "* Learning_rate can be set to 'optimal', where the learning is slightly\n",
    "decreased as more and more updates are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "# Processing the data\n",
    "import pandas as pd\n",
    "n_rows = 1_000_000\n",
    "df = pd.read_csv(\"./dataset/train.csv\", nrows = n_rows)\n",
    "\n",
    "# Splitting the column features from the target values\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "y = df['click'].values\n",
    "\n",
    "# We will only train the model using 100,000 samples\n",
    "n_train = 800_000\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "# Performing one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "X_train_enc = enc.fit_transform(X_train)\n",
    "X_test_enc = enc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# Preparing the classifier:\n",
    "sgd_lr = SGDClassifier(loss = 'log_loss', penalty = None, fit_intercept = True,\n",
    "                       max_iter = 30, learning_rate = 'constant', eta0 = 0.01,\n",
    "                       verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3.72, NNZs: 5725, Bias: -0.299019, T: 100000, Avg. loss: 0.423011\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.19, NNZs: 5725, Bias: -0.328566, T: 200000, Avg. loss: 0.416346\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.29, NNZs: 5725, Bias: -0.340531, T: 300000, Avg. loss: 0.414005\n",
      "Total training time: 3.82 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.28, NNZs: 5725, Bias: -0.292990, T: 400000, Avg. loss: 0.412099\n",
      "Total training time: 5.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.14, NNZs: 5725, Bias: -0.323408, T: 500000, Avg. loss: 0.411123\n",
      "Total training time: 6.26 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 8.93, NNZs: 5725, Bias: -0.310441, T: 600000, Avg. loss: 0.410106\n",
      "Total training time: 7.47 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 9.63, NNZs: 5725, Bias: -0.325266, T: 700000, Avg. loss: 0.409001\n",
      "Total training time: 8.69 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 10.33, NNZs: 5725, Bias: -0.321460, T: 800000, Avg. loss: 0.408316\n",
      "Total training time: 9.91 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 10.99, NNZs: 5725, Bias: -0.328541, T: 900000, Avg. loss: 0.407622\n",
      "Total training time: 11.14 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 11.62, NNZs: 5725, Bias: -0.324123, T: 1000000, Avg. loss: 0.407225\n",
      "Total training time: 12.37 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 12.24, NNZs: 5725, Bias: -0.363259, T: 1100000, Avg. loss: 0.406564\n",
      "Total training time: 13.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 12.80, NNZs: 5725, Bias: -0.370947, T: 1200000, Avg. loss: 0.406071\n",
      "Total training time: 14.81 seconds.\n",
      "Convergence after 12 epochs took 14.81 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'average': False,\n",
       " 'class_weight': None,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 0.1,\n",
       " 'eta0': 0.01,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'learning_rate': 'constant',\n",
       " 'loss': 'log_loss',\n",
       " 'max_iter': 30,\n",
       " 'n_iter_no_change': 5,\n",
       " 'n_jobs': None,\n",
       " 'penalty': None,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'tol': 0.001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 1,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model:\n",
    "sgd_lr.fit(X_train_enc.toarray(), y_train)\n",
    "\n",
    "sgd_lr.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 100000, AUC on testing set: 0.725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pred = sgd_lr.predict_proba(X_test_enc.toarray())[:, 1]\n",
    "print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(y_test, pred):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using L1 Regularization\n",
    "* Regularization type is specified using the penalty parameter in scikit-learn.\n",
    "* L1 - Lasso enables feature selection by allowing some weights with a\n",
    "significantly small value and some with a significantly large value (L2\n",
    "penalizes small and large values), which makes it easy to identify those\n",
    "features that do not have much effect on minimizing the cost function.\n",
    "* The parameter $\\alpha$ provides a trade-off between log loss and\n",
    "generalization. If $\\alpha$ is too small, it is not able to compress large\n",
    "weights and the model may suffer from high variance or overfitting; on the other hand, if α is too large, the model may become over generalized and perform poorly in terms of fitting the dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(eta0=0.01, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;, max_iter=10,\n",
       "              penalty=&#x27;l1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(eta0=0.01, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;, max_iter=10,\n",
       "              penalty=&#x27;l1&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(eta0=0.01, learning_rate='constant', loss='log_loss', max_iter=10,\n",
       "              penalty='l1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to retrain the model to find the features and enable the penalty 'l1'\n",
    "sgd_lr_l1 = SGDClassifier(loss = 'log_loss', penalty = 'l1', alpha = 0.0001,\n",
    "                          fit_intercept = True, max_iter = 10, learning_rate\n",
    "                          = 'constant', eta0 = 0.01)\n",
    "sgd_lr_l1.fit(X_train_enc.toarray(), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "The 10 least important features are:\n",
      " ['x0_1001' 'x8_84a9d4ba' 'x8_84915a27' 'x8_8441e1f3' 'x8_840161a0'\n",
      " 'x8_83fbdb80' 'x8_83fb63cd' 'x8_83ed0b87' 'x8_83cd1c10' 'x8_83ca6fdb']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Checking the absolute values of the coefficients\n",
    "coef_abs = np.abs(sgd_lr_l1.coef_)\n",
    "\n",
    "# Getting the bottom 10 coefficients\n",
    "bottom_10 = np.argsort(coef_abs)[0][:10]\n",
    "\n",
    "# Printing the values\n",
    "print(np.sort(coef_abs)[0][:10])\n",
    "\n",
    "# Printing the feature names\n",
    "feature_names = enc.get_feature_names_out()\n",
    "print(f\"The 10 least important features are:\\n {feature_names[bottom_10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are 1001 from the 0 column (that is the C1 column) in X_train, \"851897aa\" from the 8 column (that is the device_model column), and so on and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71606761 0.72316631 0.78642518 0.83673987 0.92622106 1.05117715\n",
      " 1.0697904  1.09063128 1.10990209 1.32322116]\n",
      "The 10 most important features are:\n",
      " ['x3_7687a86e' 'x4_28905ebd' 'x18_15' 'x18_61' 'x5_5e3f096f' 'x5_9c13b419'\n",
      " 'x2_763a42b5' 'x3_27e3c518' 'x2_d9750ee7' 'x5_1779deee']\n"
     ]
    }
   ],
   "source": [
    "# Now getting the top 10 coefficients\n",
    "print(np.sort(coef_abs)[0][-10:])\n",
    "\n",
    "# Getting the values in a variable\n",
    "top_10 = np.argsort(coef_abs)[0][-10:]\n",
    "\n",
    "# Printing the feature names\n",
    "print(f\"The 10 most important features are:\\n {feature_names[top_10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are \"cef3e649\" from the 7 column (that is app_category) in X_train, \"7687a86e\" from the third column (that is site_domain), and so on and so forth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Learning\n",
    "* We'll use the partial_fit() method to train the model with 100_000 samples\n",
    "at a time, which reduces the computational effort of feeding the complete\n",
    "dataset (meaning that we don't have to retrain the model entirely if we want\n",
    "to add new data).\n",
    "* It allows to train models with real-time data.\n",
    "* This time, we'll feed the model 1_000_000 samples, so we need to redefine\n",
    "our training and testing data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining our variables\n",
    "n_rows = 100_000 * 11\n",
    "df = pd.read_csv(\"./dataset/train.csv\", nrows = n_rows)\n",
    "# Splitting the features from the target\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "y = df['click'].values\n",
    "\n",
    "# Splitting in training and testing\n",
    "Y = df['click'].values\n",
    "n_train = 100000 * 10\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the SGD model. max_iter is set to 1 for online learning\n",
    "sgd_lr_online = SGDClassifier(loss = 'log_loss', penalty = 'l1',\n",
    "                              fit_intercept = True, max_iter = 1,\n",
    "                              learning_rate = 'constant', eta0 = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 105.16404340000008.3fs seconds ---\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "# Building a loop (10 times). We need to specify the classes in online learning\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(10):\n",
    "    x_train = X_train[i * 100_000: (i+1) * 100_000]\n",
    "    y_train = Y_train[i * 100_000: (i+1) * 100_000]\n",
    "    x_train_enc = enc.transform(x_train)\n",
    "    sgd_lr_online.partial_fit(x_train_enc.toarray(), y_train, classes = [0, 1])\n",
    "\n",
    "print(f\"--- {(timeit.default_timer() - start_time)}.3fs seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 10000000, AUC on testing set: 0.754\n"
     ]
    }
   ],
   "source": [
    "# Applying the trained model on the testing set, the final 100_000 samples\n",
    "x_test_enc = enc.transform(X_test)\n",
    "\n",
    "pred = sgd_lr_online.predict_proba(x_test_enc.toarray())[:, 1]\n",
    "print(f'Training samples: {n_train * 10}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
